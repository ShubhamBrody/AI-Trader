{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23476106",
   "metadata": {},
   "source": [
    "# Train on Google Colab (Cloud GPU)\n",
    "\n",
    "This notebook is the **cloud runbook** for training all models in one go, without running the API server.\n",
    "\n",
    "**What you upload first**\n",
    "- Your SQLite DB file `data/app.db` (must contain `candles` + `instrument_meta`).\n",
    "\n",
    "**What you download after training**\n",
    "- `data/app.db` (contains ridge + deep models inside `trained_models` table)\n",
    "- `data/models/pattern_seq.pt` (sequence-based candlestick pattern model)\n",
    "\n",
    "Security:\n",
    "- Don’t paste tokens into notebook outputs (use Secrets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053f843",
   "metadata": {},
   "source": [
    "## 1) Pick a Free Cloud Notebook (Colab/Kaggle/others)\n",
    "\n",
    "**Options**\n",
    "\n",
    "| Platform | Free GPU? | Typical session limits | Notes |\n",
    "|---|---:|---|---|\n",
    "| Google Colab (Free) | Sometimes | ~2–12h, may disconnect | Best docs + Drive mount |\n",
    "| Kaggle Notebooks | Often | ~9h GPU weekly quota | Great for long-ish jobs; requires Kaggle secrets |\n",
    "| Other “free GPU” sites | Unreliable | Varies | Often limited or disappears |\n",
    "\n",
    "For your “train ALL NSE_EQ” job, **free tiers are not guaranteed** to finish in one session. The only practical way on free tiers is:\n",
    "- run with **checkpointing/resume**, and\n",
    "- continue over multiple sessions (or upgrade to paid for stability).\n",
    "\n",
    "Next cell prints runtime info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a42955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, platform, textwrap\n",
    "\n",
    "print('python:', sys.version)\n",
    "print('platform:', platform.platform())\n",
    "print('cwd:', os.getcwd())\n",
    "print('pid:', os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde7458",
   "metadata": {},
   "source": [
    "## 2) Open a Hosted Runtime (GPU/TPU/CPU) and Verify Hardware\n",
    "\n",
    "- **Colab**: Runtime → Change runtime type → Hardware accelerator → **GPU**.\n",
    "- **Kaggle**: Notebook settings → Accelerator → **GPU**.\n",
    "\n",
    "Next cells detect GPU/TPU and PyTorch CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111bde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check (works in Colab/Kaggle)\n",
    "import subprocess, shutil\n",
    "\n",
    "if shutil.which('nvidia-smi'):\n",
    "    subprocess.run(['nvidia-smi'])\n",
    "else:\n",
    "    print('nvidia-smi not found (CPU runtime or TPU-only runtime).')\n",
    "\n",
    "# TPU hint (Colab)\n",
    "print('TPU_NAME:', os.environ.get('TPU_NAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846398bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch CUDA check\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    print('torch:', torch.__version__)\n",
    "    print('cuda_available:', torch.cuda.is_available())\n",
    "    print('device_count:', torch.cuda.device_count())\n",
    "    if torch.cuda.is_available():\n",
    "        print('device0:', torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print('torch import failed:', type(e).__name__, str(e)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba114dad",
   "metadata": {},
   "source": [
    "## 3) Mount/Access Data (Google Drive / Kaggle Datasets / Direct Download)\n",
    "\n",
    "For your backend training job, you want persistence for:\n",
    "- SQLite DB (`DATABASE_PATH`) that stores instruments + job resume cursor\n",
    "- Model artifacts (whatever your project writes under `data/`)\n",
    "\n",
    "### Option A: Google Drive (Colab)\n",
    "Mount Drive and use a folder like `MyDrive/DemoBackendAi/`.\n",
    "\n",
    "### Option B: Kaggle Datasets\n",
    "If you later export candles to a Kaggle dataset, you can mount it under `/kaggle/input/...`.\n",
    "\n",
    "### Option C: Direct download\n",
    "Useful for non-auth data; shown here with checksum verification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa29ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Colab Drive mount\n",
    "# (If you're in Kaggle, skip this cell.)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = '/content/drive/MyDrive/DemoBackendAi'\n",
    "except Exception as e:\n",
    "    # Not in Colab\n",
    "    BASE_DIR = '/content/DemoBackendAi'\n",
    "    print('Drive mount skipped:', type(e).__name__)\n",
    "\n",
    "import os\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "print('BASE_DIR=', BASE_DIR)\n",
    "\n",
    "# Use one DATA_DIR everywhere\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print('DATA_DIR=', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option C: Direct download with checksum (example template)\n",
    "# This is just a template; your Upstox candle fetch uses authenticated API calls instead.\n",
    "import hashlib, urllib.request\n",
    "\n",
    "def sha256_file(path: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# Example usage (disabled by default)\n",
    "EXAMPLE_URL = None  # e.g. 'https://example.com/file.bin'\n",
    "EXAMPLE_SHA256 = None\n",
    "\n",
    "if EXAMPLE_URL and EXAMPLE_SHA256:\n",
    "    out_path = os.path.join(DATA_DIR, 'download.bin')\n",
    "    urllib.request.urlretrieve(EXAMPLE_URL, out_path)\n",
    "    digest = sha256_file(out_path)\n",
    "    print('sha256:', digest)\n",
    "    assert digest == EXAMPLE_SHA256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb20cd",
   "metadata": {},
   "source": [
    "## 4) Install Dependencies\n",
    "\n",
    "In Colab, install normal deps first. Install PyTorch only if it’s missing (Colab usually includes it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Choose ONE: clone from git OR upload zip ----\n",
    "# Option 1: git clone (recommended)\n",
    "REPO_URL = ''  # <- paste your repo URL (GitHub/GitLab). Prefer a PRIVATE repo.\n",
    "REPO_DIR = os.path.join(BASE_DIR, 'repo')\n",
    "\n",
    "if REPO_URL:\n",
    "    import shutil, subprocess\n",
    "    if os.path.exists(REPO_DIR) and os.listdir(REPO_DIR):\n",
    "        print('Repo already exists:', REPO_DIR)\n",
    "    else:\n",
    "        os.makedirs(REPO_DIR, exist_ok=True)\n",
    "        subprocess.check_call(['bash', '-lc', f'git clone --depth 1 {REPO_URL} {REPO_DIR}'])\n",
    "\n",
    "print('REPO_DIR=', REPO_DIR)\n",
    "\n",
    "# Option 2: zip upload (Colab)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # upload DemoBackendAi.zip\n",
    "# !unzip -o DemoBackendAi.zip -d \"$REPO_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549286f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import os, subprocess, sys\n",
    "\n",
    "assert os.path.exists(os.path.join(REPO_DIR, 'requirements.txt')), 'Set REPO_DIR first (clone/unzip).'\n",
    "\n",
    "subprocess.check_call(['bash', '-lc', f'cd \"{REPO_DIR}\" && pip install -q -r requirements.txt'])\n",
    "\n",
    "# Install torch only if missing (Colab usually has CUDA torch)\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    print('torch already available')\n",
    "except Exception:\n",
    "    subprocess.check_call(['bash', '-lc', 'pip install -q torch'])\n",
    "\n",
    "import fastapi, httpx\n",
    "print('fastapi:', fastapi.__version__)\n",
    "print('httpx:', httpx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300282e",
   "metadata": {},
   "source": [
    "## 5) Upload Your Existing DB (recommended)\n",
    "\n",
    "To train on cloud, the fastest path is to upload your existing SQLite DB from your PC.\n",
    "\n",
    "That DB should already contain:\n",
    "- `candles` (historical candles)\n",
    "- `instrument_meta` (NSE_EQ universe keys)\n",
    "\n",
    "This notebook will place it at `./data/app.db` so the backend training code finds it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43226ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload DB + configure paths (Colab)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab upload helper\n",
    "try:\n",
    "    from google.colab import files\n",
    "except Exception:\n",
    "    files = None\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/models', exist_ok=True)\n",
    "\n",
    "# 1) Upload your app.db from your PC\n",
    "if files is not None:\n",
    "    uploaded = files.upload()  # upload a file named app.db\n",
    "    if 'app.db' in uploaded:\n",
    "        Path('data/app.db').write_bytes(uploaded['app.db'])\n",
    "        print('Saved DB to data/app.db')\n",
    "    else:\n",
    "        print('Upload skipped or file not named app.db')\n",
    "else:\n",
    "    print('Not running in Colab; ensure data/app.db exists manually')\n",
    "\n",
    "# 2) Make backend read the DB/model paths\n",
    "os.environ['DATABASE_PATH'] = os.path.abspath('data/app.db')\n",
    "os.environ['PATTERN_SEQ_MODEL_PATH'] = os.path.abspath('data/models/pattern_seq.pt')\n",
    "\n",
    "print('DATABASE_PATH=', os.environ['DATABASE_PATH'])\n",
    "print('PATTERN_SEQ_MODEL_PATH=', os.environ['PATTERN_SEQ_MODEL_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec926074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Hugging Face token (if you ever push models)\n",
    "# os.environ['HF_TOKEN'] = '...'\n",
    "\n",
    "# OPTIONAL: Kaggle API credential setup (if needed)\n",
    "# 1) Upload kaggle.json via notebook UI\n",
    "# 2) Then:\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets list | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234e717",
   "metadata": {},
   "source": [
    "## 6) Define Training Config (batch size, epochs, mixed precision) for Cloud Limits\n",
    "\n",
    "Free tiers are time-limited. Keep configs conservative, and checkpoint often.\n",
    "\n",
    "Effective batch size:\n",
    "\n",
    "$$\\text{effective\\_batch}=\\text{batch\\_size}\\times\\text{grad\\_accumulation}$$\n",
    "\n",
    "Below is a general config object you can tune. (Your project’s CLI already has many knobs; we’ll map them in Section 7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CloudTrainConfig:\n",
    "    batch_size: int = 128\n",
    "    grad_accumulation: int = 1\n",
    "    epochs_long: int = 3\n",
    "    epochs_intraday: int = 3\n",
    "    sleep_seconds_per_chunk: float = 0.2\n",
    "    sleep_seconds_per_symbol: float = 0.4\n",
    "    page_size: int = 200\n",
    "\n",
    "cfg = CloudTrainConfig()\n",
    "print(cfg)\n",
    "print('effective_batch=', cfg.batch_size * cfg.grad_accumulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7982fac",
   "metadata": {},
   "source": [
    "## 7) Train All Models (NO API server needed)\n",
    "\n",
    "You can train everything in one go with a single command. It prints:\n",
    "- which model family is training\n",
    "- progress percentage\n",
    "- elapsed + ETA\n",
    "\n",
    "This uses `scripts/train_all_models_local.py` (runs training directly in-process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke run (quick sanity check)\n",
    "# - trains ridge on DEFAULT_UNIVERSE\n",
    "# - trains deep on first 3 NSE_EQ symbols (GPU required)\n",
    "# - trains pattern-seq on first 3 NSE_EQ symbols\n",
    "import os, subprocess\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "cmd = (\n",
    "    'python scripts/train_all_models_local.py '\n",
    "    '--nse-max-symbols 3 '\n",
    "    '--deep-epochs 1 --ps-epochs 1 '\n",
    "    '--run-ridge-batch --run-deep-nse-eq --run-pattern-seq'\n",
    " )\n",
    "print(cmd)\n",
    "subprocess.check_call(['bash', '-lc', cmd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e614d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: confirm your DB has the needed tables + universe\n",
    "import os\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "from app.core.db import db_conn\n",
    "from app.universe.service import UniverseService\n",
    "\n",
    "with db_conn() as conn:\n",
    "    tables = [r['name'] for r in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()]\n",
    "print('tables:', sorted(tables)[:10], '... total=', len(tables))\n",
    "\n",
    "uni = UniverseService()\n",
    "print('NSE_EQ count:', uni.count(prefix='NSE_EQ|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0550122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full run: train all models (may take a long time)\n",
    "import os, subprocess\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "cmd = (\n",
    "    'python scripts/train_all_models_local.py '\n",
    "    '--nse-max-symbols 0 '\n",
    "    '--run-ridge-batch --run-deep-nse-eq --run-pattern-seq'\n",
    " )\n",
    "print(cmd)\n",
    "# Uncomment to start the full run:\n",
    "# subprocess.check_call(['bash', '-lc', cmd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have GPU, you can still train everything except deep models:\n",
    "import os, subprocess\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "cmd = (\n",
    "    'python scripts/train_all_models_local.py '\n",
    "    '--no-run-deep-nse-eq '\n",
    "    '--run-ridge-batch --run-pattern-seq'\n",
    " )\n",
    "print(cmd)\n",
    "# subprocess.check_call(['bash', '-lc', cmd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d475d",
   "metadata": {},
   "source": [
    "## 8) Download Trained Artifacts to Your PC\n",
    "\n",
    "After training finishes, download these files:\n",
    "- `data/app.db` (ridge + deep models live inside this DB)\n",
    "- `data/models/pattern_seq.pt` (pattern sequence model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f87813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifacts (Colab)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except Exception:\n",
    "    files = None\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "db_path = Path('data/app.db')\n",
    "ps_path = Path('data/models/pattern_seq.pt')\n",
    "\n",
    "print('DB exists:', db_path.exists(), db_path)\n",
    "print('pattern_seq exists:', ps_path.exists(), ps_path)\n",
    "\n",
    "if files is not None:\n",
    "    if db_path.exists():\n",
    "        files.download(str(db_path))\n",
    "    if ps_path.exists():\n",
    "        files.download(str(ps_path))\n",
    "else:\n",
    "    print('Not in Colab; download files manually from the filesystem')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
